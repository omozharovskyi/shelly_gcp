# Оновлені плани проєкту для IoT-рішень

## Інтеграція AWS S3 та Google Cloud Platform для зберігання та аналізу IoT-даних

### Цілі проєкту
- Забезпечити ефективне зберігання телеметрії з IoT-пристроїв (Shelly EM) за допомогою AWS S3.
- Використовувати інструменти Google Cloud Platform для масштабованого та ефективного аналізу даних.
- Перейти до повністю інтегрованої системи, яка підтримує як реальний час, так і пакетну обробку даних.

### Етапи реалізації

#### Етап 0: Налаштування та конфігурація сенсора
1. **Фізичне підключення пристрою**
   - Встановіть і безпечно підключіть пристрій Shelly EM до джерела живлення та мережі.
2. **Налаштування мобільного додатку**
   - Налаштуйте мобільний додаток для початкового управління сенсором.
3. **Конфігурація MQTT**
   - Налаштуйте Shelly EM для передачі телеметрії до AWS IoT Core через MQTT, забезпечуючи правильні теми й аутентифікацію.
4. **Налаштування правил AWS IoT Core**
   - Створіть правила в AWS IoT Core для пересилання даних телеметрії в Amazon S3.

#### Етап 1: Зберігання даних в Amazon S3
1. **Налаштування бакету S3**
   - Створіть і налаштуйте S3-бакет з управлінням життєвим циклом для оптимізації витрат.
   - Захистіть бакет за допомогою відповідних IAM політик для управління доступом.
2. **Автоматичне перенесення даних**
   - Налаштуйте правила AWS IoT Core для безперебійної передачі даних у S3-бакет.

#### Етап 2: Інтеграція з Google Cloud Platform
1. **Доступ до даних S3 з GCP**
   - Використовуйте Google Cloud Storage Transfer Service для регулярного імпорту даних із S3 до GCP.
   - Налаштуйте IAM ролі та сервісні акаунти для безпечного доступу.
2. **Синхронізація даних**
   - Автоматизуйте процес синхронізації даних, щоб GCP отримував щоденні агреговані файли JSONL із AWS S3.

#### Етап 3: Обробка та аналіз даних
1. **Агрегація даних**
   - Агрегуйте JSON-об'єкти у щоденні файли JSONL за допомогою AWS Lambda та State Machines.
2. **Аналіз у BigQuery**
   - Імпортуйте агреговані дані до BigQuery та створюйте SQL-запити для аналізу споживання енергії.
3. **Візуалізація даних**
   - Використовуйте Looker Studio (Google Data Studio) для створення дашбордів, які надають уявлення про споживання енергії та витрати.

### Корекції на основі попередньої роботи
- Акцент на **агрегацію даних на рівні AWS S3**, щоб мінімізувати операційні витрати в GCP.
- Використання **зовнішніх таблиць BigQuery**, щоб уникнути дублювання даних.
- Застосування **обчислень на основі інтегралів** для споживання енергії в нерівномірних часових інтервалах.

### Заходи безпеки
- Забезпечення шифрування даних під час зберігання та передачі.
- Регулярний перегляд і оновлення політик доступу відповідно до найкращих практик.

### Оптимізація ресурсів
- Оптимізація витрат на зберігання в S3 та обробку даних у GCP на основі історичного використання.
- Перенесення невикористаних сирих даних в AWS Glacier для зниження витрат.

### Резервування та відновлення
- Реалізація політик резервного копіювання для даних в AWS і GCP.
- Періодичне тестування планів відновлення для забезпечення надійності.

### Моніторинг і логування
- Налаштування CloudWatch (AWS) і Google Cloud Monitoring для відстеження стану системи.
- Логування важливих подій для діагностики та аналізу проблем.

### Версіонування та документація
- Ведення детального контролю версій конфігурацій і скриптів.
- Підготовка повної документації для забезпечення прозорості та підтримки проєкту.

### Подальші плани
- Інтеграція даних про генерацію енергії від сонячних панелей.
- Дослідження потенційної економії за рахунок зміни моделей споживання та багатозонних тарифів.
- Розробка більш просунутого аналізу за допомогою PySpark або машинного навчання для прогнозування споживання енергії.

### Заключні заходи
- Проведення повного тестування всієї системи від налаштування сенсора до аналізу даних.
- Розгортання системи у виробниче середовище з організацією тренінгів і підтримки користувачів.
- Моніторинг і вдосконалення системи на основі зібраних даних.

### Основний висновок
Оновлений план враховує уроки, отримані з попередніх реалізацій, для покращення масштабованості, економічної ефективності та можливостей аналітики, забезпечуючи надійне та зручне IoT-рішення.
